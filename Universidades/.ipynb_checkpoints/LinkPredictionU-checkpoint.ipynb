{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4svK7xD75iFn"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLcuyFGJj2KH"
   },
   "outputs": [],
   "source": [
    "#Definiendo varias metricas para link prediction\n",
    "def commonNeighbors(G,source,target):\n",
    "  adj = dict(G.adj[source])\n",
    "  adj2 = dict(G.adj[target])\n",
    "  commonNeighbors = set(adj.keys()) & set(adj2.keys())\n",
    "  return commonNeighbors\n",
    "\n",
    "def jaccardCoeff(G,source,target):\n",
    "  adj = dict(G.adj[source])\n",
    "  adj2 = dict(G.adj[target])\n",
    "  unionNeighbors = set(adj.keys()) | set(adj2.keys())\n",
    "  commonNeighbors = set(adj.keys()) & set(adj2.keys())\n",
    "  return len(commonNeighbors)/len(unionNeighbors)\n",
    "\n",
    "def adamicAdar(G,source,target):\n",
    "  cn = commonNeighbors(G,source,target)\n",
    "  coeff = 0\n",
    "  for neighbor in cn:\n",
    "    if len(dict(G.adj[neighbor]).keys()) >1:\n",
    "      coeff += 1/np.log(len(dict(G.adj[neighbor]).keys()))\n",
    "  return coeff\n",
    "\n",
    "def similarity(G,source,target):\n",
    "    adj = dict(G.adj[source])\n",
    "    adj2 = dict(G.adj[target])\n",
    "    y=0.5\n",
    "    val = 0\n",
    "    for a in set(adj.keys()):\n",
    "        for b in set(adj2.keys()):\n",
    "            if a==b:\n",
    "                val+= 1\n",
    "            else:\n",
    "                val+= similarity(G,a,b)\n",
    "    return val/(len(adj.keys())*len(adj2.keys()))\n",
    "\n",
    "def commonKeywords(nodes_df,source,target):\n",
    "  keywords1 = list(nodes_df[nodes_df[\"ID\"]==source][\"KeywordsB2018\"])\n",
    "  keywords1 = set(str(keywords1[0]).split(\",\"))\n",
    "  keywords2 = list(nodes_df[nodes_df[\"ID\"]==target][\"KeywordsB2018\"])\n",
    "  keywords2 = set(str(keywords2[0]).split(\",\"))\n",
    "  common = keywords1 & keywords2\n",
    "  return len(common)\n",
    "\n",
    "#Contador binario\n",
    "def contador(l):\n",
    "    acarreo =True\n",
    "    for i in range(len(l)):\n",
    "        if(acarreo):\n",
    "            l[i] = not(l[i])\n",
    "            acarreo = l[i]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2162129,
     "status": "ok",
     "timestamp": 1589242909123,
     "user": {
      "displayName": "Alex Velez Llaque",
      "photoUrl": "",
      "userId": "11111465116901712450"
     },
     "user_tz": 300
    },
    "id": "vcGo_UIujrrC",
    "outputId": "af8ced83-716b-475f-d235-d6aa6d27e92d"
   },
   "outputs": [],
   "source": [
    "universidades = ['ESPOL']\n",
    "predictionsDic = {}\n",
    "cont=0\n",
    "for univ in tqdm(universidades,desc='Universidades'):\n",
    "  archivo_edges = \"data/grafo_completo/coauthors-edges-\"+univ+\".csv\"\n",
    "  archivo_nodes = \"data/grafo_completo/coauthors-nodes-\"+univ+\".csv\"\n",
    "  coau_df = pd.read_csv(archivo_edges,delimiter=\";\")\n",
    "  nodos_df = pd.read_csv(archivo_nodes,delimiter=\";\")\n",
    "  G = nx.from_pandas_edgelist(coau_df,\"Source\",\"Target\",['Weight','Year'],create_using=nx.Graph())\n",
    "  nodos = list(G.nodes)\n",
    "  #Obteniendo dataframe de pares de nodos que no tienen coneccion\n",
    "\n",
    "  adj_G = nx.to_numpy_matrix(G, nodelist = nodos)\n",
    "  all_unconnected_pairs = []\n",
    "\n",
    "  offset = 0\n",
    "  for i in range(adj_G.shape[0]):\n",
    "    for j in range(offset,adj_G.shape[1]):\n",
    "      if i != j:\n",
    "            if adj_G[i,j] == 0:\n",
    "              all_unconnected_pairs.append((nodos[i],nodos[j]))\n",
    "\n",
    "    offset = offset + 1\n",
    "\n",
    "  node_1_unlinked = [i[0] for i in all_unconnected_pairs]\n",
    "  node_2_unlinked = [i[1] for i in all_unconnected_pairs]\n",
    "\n",
    "  #Creando dataframe para almacenar a los pares de nodos no conectados que se recogieron\n",
    "  data = pd.DataFrame({'Source':node_1_unlinked,'Target':node_2_unlinked})\n",
    "  data['Weight'] = 0\n",
    "  data['link'] = 0\n",
    "\n",
    "  indices = range(len(data['Source']))\n",
    "  #Separando el dataframe en datos para train y para test\n",
    "  #(se busca un numero no muy grande de datos de pares que no tienen coneccion en los datos para entrenar, para\n",
    "  #evitar que los datos de entrenamiento esten desbalanceados)\n",
    "\n",
    "  if len(indices) > len(coau_df['Source'])*5:\n",
    "    _, indices = train_test_split(indices,test_size=0.15,random_state=73)\n",
    "  removeIn, noRemoveIn = train_test_split(indices,test_size=0.25,random_state=32)\n",
    "\n",
    "  test= data.copy()\n",
    "  test = test.loc[noRemoveIn]\n",
    "\n",
    "  data = data.loc[removeIn]\n",
    "\n",
    "  #Trabajando con los pares de nodos que si estan conectados\n",
    "\n",
    "  #Obteniendo lista de los links que se pueden borrar (que no eliminan nodos, o dividen al grafo en mas de una componente) y que\n",
    "  #estan despues del aÃ±o establecido\n",
    "\n",
    "  year = 2018\n",
    "  initial_node_count = len(G.nodes)\n",
    "\n",
    "  coau_df_temp = coau_df.copy()\n",
    "\n",
    "  omissible_links_index = []\n",
    "  for i in tqdm(coau_df.index.values):\n",
    "      if G.adj[coau_df.values[i,0]][coau_df.values[i,1]]['Year']>=year:\n",
    "          # Eliminar una arista y construir un nuevo grafo sin esa unica arista\n",
    "          G_temp = nx.from_pandas_edgelist(coau_df_temp.drop(index=i), \"Source\", \"Target\",[\"Weight\"], create_using=nx.Graph())\n",
    "\n",
    "          # Verificando que al eliminar este par, no parte el grafo, y que el numero de nodos siga siendo el mismo\n",
    "\n",
    "          if (nx.number_connected_components(G_temp) == 1) and (len(G_temp.nodes) == initial_node_count):\n",
    "              omissible_links_index.append(i)\n",
    "              coau_df_temp = coau_df_temp.drop(i)\n",
    "\n",
    "  #creando dataframe de edges que se pueden remover\n",
    "\n",
    "  coau_df_temp2 = coau_df.copy()\n",
    "  coau_df_temp2[\"link\"]=1\n",
    "\n",
    "  #Separando dataframe en datos para train y test\n",
    "\n",
    "  testlinks = coau_df_temp2.loc[omissible_links_index]\n",
    "  coau_df_temp = coau_df_temp2.drop(index=omissible_links_index)\n",
    "\n",
    "  data = data.append(coau_df_temp[['Source', 'Target', 'link','Weight']], ignore_index=True)\n",
    "  data[\"Weight\"] = data[\"Weight\"].astype('int64')\n",
    "\n",
    "  test = test.append(testlinks[['Source', 'Target', 'link','Weight']], ignore_index=True)\n",
    "\n",
    "\n",
    "  #Creando nuevo grafo sin los links eliminados\n",
    "  G_data = nx.from_pandas_edgelist(coau_df_temp, \"Source\", \"Target\",[\"Weight\"], create_using=nx.Graph())\n",
    "\n",
    "  #SacandoFeatures:\n",
    "  Xtrain =[]\n",
    "  Ytrain =[]\n",
    "  for i in data.values:\n",
    "      Xtrain.append([len(commonNeighbors(G_data,i[0],i[1])),jaccardCoeff(G_data,i[0],i[1]),\n",
    "                     adamicAdar(G_data,i[0],i[1]),commonKeywords(nodos_df,i[0],i[1])])\n",
    "      Ytrain.append(i[3])\n",
    "  Xtrain = np.array(Xtrain)\n",
    "  Ytrain = np.array(Ytrain)\n",
    "\n",
    "  Xtest = []\n",
    "  Ytest = []\n",
    "  for i in test.values:\n",
    "      Xtest.append([len(commonNeighbors(G_data,i[0],i[1])),jaccardCoeff(G_data,i[0],i[1]),\n",
    "                    adamicAdar(G_data,i[0],i[1]),commonKeywords(nodos_df,i[0],i[1])])\n",
    "      Ytest.append(i[3])\n",
    "\n",
    "  Xtest = np.array(Xtest)\n",
    "  Ytest = np.array(Ytest)\n",
    "\n",
    "  #Prediciendo\n",
    "  lista = ['CommonNeighbors',\"JaccardCoeff\",\"AdamicAdar\",\"CommonKeywords\"]\n",
    "  lista = np.array(lista)\n",
    "  l = [True]*len(lista)\n",
    "  predictionsDic[univ] ={}\n",
    "  for i in range(2**len(lista)-1):\n",
    "    \n",
    "    lr = LogisticRegression(class_weight=\"balanced\")\n",
    "    lr.fit(Xtrain[:,l], Ytrain)\n",
    "\n",
    "    predictions = lr.predict(Xtest[:,l])\n",
    "    t = \"-\".join(lista[l])\n",
    "    \n",
    "    predictionsDic[univ][t] = {\n",
    "            'regresionLogistica':{\n",
    "                'accuracy':accuracy_score(Ytest,predictions),\n",
    "                'roc_auc':roc_auc_score(Ytest, predictions),\n",
    "                'recall':recall_score(Ytest,predictions),\n",
    "                'precision':precision_score(Ytest,predictions)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "                \n",
    "    clf = SVC(gamma='scale')\n",
    "\n",
    "    clf.fit(Xtrain[:,l], Ytrain)\n",
    "    predictions = clf.predict(Xtest[:,l])\n",
    "\n",
    "    predictionsDic[univ][t]['SVC'] = {\n",
    "        'accuracy':accuracy_score(Ytest,predictions),\n",
    "        'roc_auc':roc_auc_score(Ytest, predictions),\n",
    "        'recall':recall_score(Ytest,predictions),\n",
    "        'precision':precision_score(Ytest,predictions)\n",
    "    }\n",
    "\n",
    "    bagging = BaggingClassifier(base_estimator=LogisticRegression(class_weight=\"balanced\"),n_estimators=10, random_state=0).fit(Xtrain[:,l], Ytrain)\n",
    "    predictions = bagging.predict(Xtest[:,l])\n",
    "\n",
    "    predictionsDic[univ][t]['Bagging'] = {\n",
    "        'accuracy':accuracy_score(Ytest,predictions),\n",
    "        'roc_auc':roc_auc_score(Ytest, predictions),\n",
    "        'recall':recall_score(Ytest,predictions),\n",
    "        'precision':precision_score(Ytest,predictions)\n",
    "    }\n",
    "    l = contador(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1589243074818,
     "user": {
      "displayName": "Alex Velez Llaque",
      "photoUrl": "",
      "userId": "11111465116901712450"
     },
     "user_tz": 300
    },
    "id": "zvuACH6U-Bz4",
    "outputId": "970d2134-1038-4824-c2f3-0c0015cc4a08"
   },
   "outputs": [],
   "source": [
    "predictionsDic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glNeGMUBjbU4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de LinkPredictionU.ipynb",
   "provenance": [
    {
     "file_id": "1R9Q1z5q929ZCBL9QBeaB4qlirdKKlMZZ",
     "timestamp": 1589245463835
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
